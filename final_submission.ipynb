{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPUQ-WVc1NLK",
        "outputId": "fd2b9e07-e8a1-4bcb-9894-09beb79b695b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d05_KqsCbyrP",
        "outputId": "3630ed32-6a08-4a7c-abd7-de99730845ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting protobuf==3.9.2\n",
            "  Downloading protobuf-3.9.2-py2.py3-none-any.whl (431 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.5/431.5 KB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.9.2) (57.4.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.9.2) (1.15.0)\n",
            "Installing collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.19.6\n",
            "    Uninstalling protobuf-3.19.6:\n",
            "      Successfully uninstalled protobuf-3.19.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.12.0 requires protobuf<4,>=3.13, but you have protobuf 3.9.2 which is incompatible.\n",
            "tensorflow-datasets 4.8.2 requires protobuf>=3.12.2, but you have protobuf 3.9.2 which is incompatible.\n",
            "proto-plus 1.22.2 requires protobuf<5.0.0dev,>=3.19.0, but you have protobuf 3.9.2 which is incompatible.\n",
            "grpcio-status 1.48.2 requires protobuf>=3.12.0, but you have protobuf 3.9.2 which is incompatible.\n",
            "googleapis-common-protos 1.58.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.9.2 which is incompatible.\n",
            "google-cloud-translate 3.8.4 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.9.2 which is incompatible.\n",
            "google-cloud-language 2.6.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.9.2 which is incompatible.\n",
            "google-cloud-firestore 2.7.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.9.2 which is incompatible.\n",
            "google-cloud-datastore 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.9.2 which is incompatible.\n",
            "google-cloud-bigquery 3.4.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.9.2 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.18.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.9.2 which is incompatible.\n",
            "google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.9.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-3.9.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting paddlepaddle\n",
            "  Downloading paddlepaddle-2.4.1-cp38-cp38-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opt-einsum==3.3.0 in /usr/local/lib/python3.8/dist-packages (from paddlepaddle) (3.3.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from paddlepaddle) (4.4.2)\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.8/dist-packages (from paddlepaddle) (0.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from paddlepaddle) (1.15.0)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.8/dist-packages (from paddlepaddle) (2.25.1)\n",
            "Requirement already satisfied: numpy>=1.13 in /usr/local/lib/python3.8/dist-packages (from paddlepaddle) (1.21.6)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from paddlepaddle) (7.1.2)\n",
            "Collecting paddle-bfloat==0.1.7\n",
            "  Downloading paddle_bfloat-0.1.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (385 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m385.5/385.5 KB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf<=3.20.0,>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from paddlepaddle) (3.9.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf<=3.20.0,>=3.1.0->paddlepaddle) (57.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20.0->paddlepaddle) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20.0->paddlepaddle) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20.0->paddlepaddle) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20.0->paddlepaddle) (1.24.3)\n",
            "Installing collected packages: paddle-bfloat, paddlepaddle\n",
            "Successfully installed paddle-bfloat-0.1.7 paddlepaddle-2.4.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting paddleocr>=2.0.1\n",
            "  Downloading paddleocr-2.6.1.3-py3-none-any.whl (445 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m445.9/445.9 KB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: imgaug in /usr/local/lib/python3.8/dist-packages (from paddleocr>=2.0.1) (0.4.0)\n",
            "Collecting PyMuPDF<1.21.0\n",
            "  Downloading PyMuPDF-1.20.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyclipper\n",
            "  Downloading pyclipper-1.3.0.post4-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (619 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.2/619.2 KB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting premailer\n",
            "  Downloading premailer-3.10.0-py2.py3-none-any.whl (19 kB)\n",
            "Collecting python-docx\n",
            "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m105.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fire>=0.3.0\n",
            "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 KB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from paddleocr>=2.0.1) (4.6.3)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.8/dist-packages (from paddleocr>=2.0.1) (0.29.33)\n",
            "Collecting fonttools>=4.24.0\n",
            "  Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m965.4/965.4 KB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdf2docx\n",
            "  Downloading pdf2docx-0.5.6-py3-none-any.whl (148 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.4/148.4 KB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz\n",
            "  Downloading rapidfuzz-2.13.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opencv-contrib-python<=4.6.0.66 in /usr/local/lib/python3.8/dist-packages (from paddleocr>=2.0.1) (4.6.0.66)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.8/dist-packages (from paddleocr>=2.0.1) (3.0.10)\n",
            "Collecting visualdl\n",
            "  Downloading visualdl-2.5.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from paddleocr>=2.0.1) (1.21.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.8/dist-packages (from paddleocr>=2.0.1) (4.9.2)\n",
            "Requirement already satisfied: lmdb in /usr/local/lib/python3.8/dist-packages (from paddleocr>=2.0.1) (0.99)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.8/dist-packages (from paddleocr>=2.0.1) (0.18.3)\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.8/dist-packages (from paddleocr>=2.0.1) (2.0.1)\n",
            "Collecting attrdict\n",
            "  Downloading attrdict-2.0.1-py2.py3-none-any.whl (9.9 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from paddleocr>=2.0.1) (4.64.1)\n",
            "Requirement already satisfied: opencv-python<=4.6.0.66 in /usr/local/lib/python3.8/dist-packages (from paddleocr>=2.0.1) (4.6.0.66)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from fire>=0.3.0->paddleocr>=2.0.1) (1.15.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.8/dist-packages (from fire>=0.3.0->paddleocr>=2.0.1) (2.2.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from imgaug->paddleocr>=2.0.1) (7.1.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from imgaug->paddleocr>=2.0.1) (3.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from imgaug->paddleocr>=2.0.1) (1.7.3)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.8/dist-packages (from imgaug->paddleocr>=2.0.1) (2.9.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.8/dist-packages (from scikit-image->paddleocr>=2.0.1) (2023.2.3)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image->paddleocr>=2.0.1) (3.0)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image->paddleocr>=2.0.1) (1.4.1)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.8/dist-packages (from openpyxl->paddleocr>=2.0.1) (1.1.0)\n",
            "Collecting cssutils\n",
            "  Downloading cssutils-2.6.0-py3-none-any.whl (399 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.7/399.7 KB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from premailer->paddleocr>=2.0.1) (2.25.1)\n",
            "Collecting cssselect\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.8/dist-packages (from premailer->paddleocr>=2.0.1) (5.3.0)\n",
            "Collecting gradio\n",
            "  Downloading gradio-3.18.0-py3-none-any.whl (14.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.2/14.2 MB\u001b[0m \u001b[31m93.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from visualdl->paddleocr>=2.0.1) (1.3.5)\n",
            "Collecting x2paddle\n",
            "  Downloading x2paddle-1.4.0-py3-none-any.whl (319 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.1/319.1 KB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rarfile\n",
            "  Downloading rarfile-4.0-py3-none-any.whl (28 kB)\n",
            "Collecting onnx>=1.6.0\n",
            "  Downloading onnx-1.13.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m96.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flask>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from visualdl->paddleocr>=2.0.1) (1.1.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from visualdl->paddleocr>=2.0.1) (23.0)\n",
            "Collecting Flask-Babel>=3.0.0\n",
            "  Downloading flask_babel-3.0.1-py3-none-any.whl (11 kB)\n",
            "Collecting protobuf>=3.11.0\n",
            "  Downloading protobuf-4.21.12-cp37-abi3-manylinux2014_x86_64.whl (409 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.8/409.8 KB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bce-python-sdk\n",
            "  Downloading bce_python_sdk-0.8.79-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.8/207.8 KB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from visualdl->paddleocr>=2.0.1) (5.4.8)\n",
            "Collecting tritonclient[all]\n",
            "  Downloading tritonclient-2.30.0-py3-none-manylinux1_x86_64.whl (11.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m103.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 KB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.8/dist-packages (from flask>=1.1.1->visualdl->paddleocr>=2.0.1) (7.1.2)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.8/dist-packages (from flask>=1.1.1->visualdl->paddleocr>=2.0.1) (1.0.1)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.8/dist-packages (from flask>=1.1.1->visualdl->paddleocr>=2.0.1) (1.1.0)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.8/dist-packages (from flask>=1.1.1->visualdl->paddleocr>=2.0.1) (2.11.3)\n",
            "Requirement already satisfied: pytz<2023.0,>=2022.7 in /usr/local/lib/python3.8/dist-packages (from Flask-Babel>=3.0.0->visualdl->paddleocr>=2.0.1) (2022.7.1)\n",
            "Collecting Flask-Babel>=3.0.0\n",
            "  Downloading flask_babel-3.0.0-py3-none-any.whl (11 kB)\n",
            "INFO: pip is looking at multiple versions of flask to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting flask>=1.1.1\n",
            "  Downloading Flask-2.2.2-py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.5/101.5 KB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting click>=8.0\n",
            "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.6/96.6 KB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Jinja2>=3.0\n",
            "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.1/133.1 KB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting itsdangerous>=2.0\n",
            "  Downloading itsdangerous-2.1.2-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: importlib-metadata>=3.6.0 in /usr/local/lib/python3.8/dist-packages (from flask>=1.1.1->visualdl->paddleocr>=2.0.1) (6.0.0)\n",
            "Collecting Werkzeug>=2.2.2\n",
            "  Downloading Werkzeug-2.2.2-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.7/232.7 KB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Babel<3.0.0,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from Flask-Babel>=3.0.0->visualdl->paddleocr>=2.0.1) (2.11.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from Jinja2>=3.0->flask>=1.1.1->visualdl->paddleocr>=2.0.1) (2.0.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->imgaug->paddleocr>=2.0.1) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->imgaug->paddleocr>=2.0.1) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->imgaug->paddleocr>=2.0.1) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->imgaug->paddleocr>=2.0.1) (0.11.0)\n",
            "Collecting protobuf>=3.11.0\n",
            "  Downloading protobuf-3.20.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.8/dist-packages (from onnx>=1.6.0->visualdl->paddleocr>=2.0.1) (4.4.0)\n",
            "Requirement already satisfied: future>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from bce-python-sdk->visualdl->paddleocr>=2.0.1) (0.16.0)\n",
            "Collecting pycryptodome>=3.8.0\n",
            "  Downloading pycryptodome-3.17-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi\n",
            "  Downloading fastapi-0.91.0-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.2/56.2 KB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy\n",
            "  Downloading ffmpy-0.3.0.tar.gz (4.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting orjson\n",
            "  Downloading orjson-3.8.6-cp38-cp38-manylinux_2_28_x86_64.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.7/140.7 KB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles\n",
            "  Downloading aiofiles-23.1.0-py3-none-any.whl (14 kB)\n",
            "Collecting python-multipart\n",
            "  Downloading python-multipart-0.0.5.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.20.0-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 KB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from gradio->visualdl->paddleocr>=2.0.1) (6.0)\n",
            "Collecting httpx\n",
            "  Downloading httpx-0.23.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 KB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from gradio->visualdl->paddleocr>=2.0.1) (3.8.3)\n",
            "Requirement already satisfied: altair>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from gradio->visualdl->paddleocr>=2.0.1) (4.2.2)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.8/dist-packages (from gradio->visualdl->paddleocr>=2.0.1) (1.10.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.8/dist-packages (from gradio->visualdl->paddleocr>=2.0.1) (2023.1.0)\n",
            "Collecting markdown-it-py[linkify,plugins]>=2.0.0\n",
            "  Downloading markdown_it_py-2.1.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 KB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting websockets>=10.0\n",
            "  Downloading websockets-10.4-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.0/107.0 KB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: dill>=0.3.6 in /usr/local/lib/python3.8/dist-packages (from multiprocess->visualdl->paddleocr>=2.0.1) (0.3.6)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->premailer->paddleocr>=2.0.1) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->premailer->paddleocr>=2.0.1) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->premailer->paddleocr>=2.0.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->premailer->paddleocr>=2.0.1) (2.10)\n",
            "Collecting python-rapidjson>=0.9.1\n",
            "  Downloading python_rapidjson-1.9-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting geventhttpclient<=2.0.2,>=1.4.4\n",
            "  Downloading geventhttpclient-2.0.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (100 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.9/100.9 KB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.41.0 in /usr/local/lib/python3.8/dist-packages (from tritonclient[all]->visualdl->paddleocr>=2.0.1) (1.51.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from x2paddle->visualdl->paddleocr>=2.0.1) (1.7.1)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio->visualdl->paddleocr>=2.0.1) (2.1.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio->visualdl->paddleocr>=2.0.1) (6.0.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio->visualdl->paddleocr>=2.0.1) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio->visualdl->paddleocr>=2.0.1) (1.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio->visualdl->paddleocr>=2.0.1) (1.8.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio->visualdl->paddleocr>=2.0.1) (4.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio->visualdl->paddleocr>=2.0.1) (22.2.0)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.8/dist-packages (from altair>=4.2.0->gradio->visualdl->paddleocr>=2.0.1) (4.3.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.8/dist-packages (from altair>=4.2.0->gradio->visualdl->paddleocr>=2.0.1) (0.12.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.8/dist-packages (from altair>=4.2.0->gradio->visualdl->paddleocr>=2.0.1) (0.4)\n",
            "Collecting brotli\n",
            "  Downloading Brotli-1.0.9-cp38-cp38-manylinux1_x86_64.whl (357 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m357.2/357.2 KB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gevent>=0.13\n",
            "  Downloading gevent-22.10.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m103.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=3.6.0->flask>=1.1.1->visualdl->paddleocr>=2.0.1) (3.12.1)\n",
            "Collecting mdurl~=0.1\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Collecting mdit-py-plugins\n",
            "  Downloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 KB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting linkify-it-py~=1.0\n",
            "  Downloading linkify_it_py-1.0.3-py3-none-any.whl (19 kB)\n",
            "Collecting MarkupSafe>=2.0\n",
            "  Downloading MarkupSafe-2.1.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Collecting starlette<0.25.0,>=0.24.0\n",
            "  Downloading starlette-0.24.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 KB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sniffio\n",
            "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Collecting rfc3986[idna2008]<2,>=1.3\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting httpcore<0.17.0,>=0.15.0\n",
            "  Downloading httpcore-0.16.3-py3-none-any.whl (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.6/69.6 KB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->x2paddle->visualdl->paddleocr>=2.0.1) (1.2.1)\n",
            "Collecting h11>=0.8\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 KB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting zope.interface\n",
            "  Downloading zope.interface-5.5.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (261 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.4/261.4 KB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: greenlet>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from gevent>=0.13->geventhttpclient<=2.0.2,>=1.4.4->tritonclient[all]->visualdl->paddleocr>=2.0.1) (2.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from gevent>=0.13->geventhttpclient<=2.0.2,>=1.4.4->tritonclient[all]->visualdl->paddleocr>=2.0.1) (57.4.0)\n",
            "Collecting zope.event\n",
            "  Downloading zope.event-4.6-py2.py3-none-any.whl (6.8 kB)\n",
            "Collecting anyio<5.0,>=3.0\n",
            "  Downloading anyio-3.6.2-py3-none-any.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.6/80.6 KB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0->altair>=4.2.0->gradio->visualdl->paddleocr>=2.0.1) (0.19.3)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0->altair>=4.2.0->gradio->visualdl->paddleocr>=2.0.1) (5.10.2)\n",
            "Collecting uc-micro-py\n",
            "  Downloading uc_micro_py-1.0.1-py3-none-any.whl (6.2 kB)\n",
            "Building wheels for collected packages: fire, python-docx, ffmpy, python-multipart\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116949 sha256=71ecb00c4817dc989e9320caa7a04a79a6f666febb7db14d77912745b27510ba\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/eb/43/7295e71293b218ddfd627f935229bf54af9018add7fbb5aac6\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184505 sha256=861402419ce862f7f8e0d84df09111669305aba007b15ecdce3c867d7be012ac\n",
            "  Stored in directory: /root/.cache/pip/wheels/32/b8/b2/c4c2b95765e615fe139b0b17b5ea7c0e1b6519b0a9ec8fb34d\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4711 sha256=97d21784638b0ade6bd03f7a5cf40e3e7a0a63b553f2237ddff856b0a27d72ec\n",
            "  Stored in directory: /root/.cache/pip/wheels/ff/5b/59/913b443e7369dc04b61f607a746b6f7d83fb65e2e19fcc958d\n",
            "  Building wheel for python-multipart (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-multipart: filename=python_multipart-0.0.5-py3-none-any.whl size=31678 sha256=bf332a7ed54324a47437c446d7eb2aa2e335ae7b7de720c550741de643cd7d69\n",
            "  Stored in directory: /root/.cache/pip/wheels/9e/fc/1c/cf980e6413d3ee8e70cd8f39e2366b0f487e3e221aeb452eb0\n",
            "Successfully built fire python-docx ffmpy python-multipart\n",
            "Installing collected packages: rfc3986, rarfile, pydub, pyclipper, ffmpy, brotli, zope.interface, zope.event, websockets, uc-micro-py, sniffio, rapidfuzz, python-rapidjson, python-multipart, python-docx, PyMuPDF, pycryptodome, protobuf, orjson, multiprocess, mdurl, MarkupSafe, itsdangerous, h11, fonttools, fire, cssutils, cssselect, click, attrdict, aiofiles, x2paddle, Werkzeug, uvicorn, tritonclient, premailer, pdf2docx, onnx, markdown-it-py, linkify-it-py, Jinja2, gevent, bce-python-sdk, anyio, starlette, mdit-py-plugins, httpcore, geventhttpclient, flask, httpx, Flask-Babel, fastapi, gradio, visualdl, paddleocr\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.9.2\n",
            "    Uninstalling protobuf-3.9.2:\n",
            "      Successfully uninstalled protobuf-3.9.2\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 2.0.1\n",
            "    Uninstalling MarkupSafe-2.0.1:\n",
            "      Successfully uninstalled MarkupSafe-2.0.1\n",
            "  Attempting uninstall: itsdangerous\n",
            "    Found existing installation: itsdangerous 1.1.0\n",
            "    Uninstalling itsdangerous-1.1.0:\n",
            "      Successfully uninstalled itsdangerous-1.1.0\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 7.1.2\n",
            "    Uninstalling click-7.1.2:\n",
            "      Successfully uninstalled click-7.1.2\n",
            "  Attempting uninstall: Werkzeug\n",
            "    Found existing installation: Werkzeug 1.0.1\n",
            "    Uninstalling Werkzeug-1.0.1:\n",
            "      Successfully uninstalled Werkzeug-1.0.1\n",
            "  Attempting uninstall: Jinja2\n",
            "    Found existing installation: Jinja2 2.11.3\n",
            "    Uninstalling Jinja2-2.11.3:\n",
            "      Successfully uninstalled Jinja2-2.11.3\n",
            "  Attempting uninstall: flask\n",
            "    Found existing installation: Flask 1.1.4\n",
            "    Uninstalling Flask-1.1.4:\n",
            "      Successfully uninstalled Flask-1.1.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
            "paddlepaddle 2.4.1 requires protobuf<=3.20.0,>=3.1.0, but you have protobuf 3.20.3 which is incompatible.\n",
            "notebook 5.7.16 requires jinja2<=3.0.0, but you have jinja2 3.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Flask-Babel-3.0.1 Jinja2-3.1.2 MarkupSafe-2.1.2 PyMuPDF-1.20.2 Werkzeug-2.2.2 aiofiles-23.1.0 anyio-3.6.2 attrdict-2.0.1 bce-python-sdk-0.8.79 brotli-1.0.9 click-8.1.3 cssselect-1.2.0 cssutils-2.6.0 fastapi-0.91.0 ffmpy-0.3.0 fire-0.5.0 flask-2.2.2 fonttools-4.38.0 gevent-22.10.2 geventhttpclient-2.0.2 gradio-3.18.0 h11-0.14.0 httpcore-0.16.3 httpx-0.23.3 itsdangerous-2.1.2 linkify-it-py-1.0.3 markdown-it-py-2.1.0 mdit-py-plugins-0.3.3 mdurl-0.1.2 multiprocess-0.70.14 onnx-1.13.0 orjson-3.8.6 paddleocr-2.6.1.3 pdf2docx-0.5.6 premailer-3.10.0 protobuf-3.20.3 pyclipper-1.3.0.post4 pycryptodome-3.17 pydub-0.25.1 python-docx-0.8.11 python-multipart-0.0.5 python-rapidjson-1.9 rapidfuzz-2.13.7 rarfile-4.0 rfc3986-1.5.0 sniffio-1.3.0 starlette-0.24.0 tritonclient-2.30.0 uc-micro-py-1.0.1 uvicorn-0.20.0 visualdl-2.5.0 websockets-10.4 x2paddle-1.4.0 zope.event-4.6 zope.interface-5.5.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting segmentation-models\n",
            "  Downloading segmentation_models-1.0.1-py3-none-any.whl (33 kB)\n",
            "Collecting image-classifiers==1.0.0\n",
            "  Downloading image_classifiers-1.0.0-py3-none-any.whl (19 kB)\n",
            "Collecting efficientnet==1.0.0\n",
            "  Downloading efficientnet-1.0.0-py3-none-any.whl (17 kB)\n",
            "Collecting keras-applications<=1.0.8,>=1.0.7\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 KB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-image in /usr/local/lib/python3.8/dist-packages (from efficientnet==1.0.0->segmentation-models) (0.18.3)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.8/dist-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation-models) (1.21.6)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.8/dist-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation-models) (3.1.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.8/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (2023.2.3)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (1.7.3)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (3.2.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (2.9.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (3.0)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (7.1.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (1.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models) (0.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models) (1.15.0)\n",
            "Installing collected packages: keras-applications, image-classifiers, efficientnet, segmentation-models\n",
            "Successfully installed efficientnet-1.0.0 image-classifiers-1.0.0 keras-applications-1.0.8 segmentation-models-1.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install protobuf==3.9.2\n",
        "!pip install paddlepaddle\n",
        "!pip install \"paddleocr>=2.0.1\"\n",
        "!pip install segmentation-models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1U7bOb7a1RKE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob as glob\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import argparse\n",
        "import time\n",
        "from pathlib import Path\n",
        "import tensorflow as tf\n",
        "# import segmentation_models as sm\n",
        "import glob\n",
        "import keras\n",
        "import cv2\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "from numpy import random\n",
        "from google.colab.patches import cv2_imshow\n",
        "from PIL import Image\n",
        "from PIL import ImageFont\n",
        "from paddleocr import PaddleOCR,draw_ocr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIgBZHHRcD7Y",
        "outputId": "02e228aa-e5ce-447c-abb2-5e553401197b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download https://paddleocr.bj.bcebos.com/PP-OCRv3/english/en_PP-OCRv3_det_infer.tar to /root/.paddleocr/whl/det/en/en_PP-OCRv3_det_infer/en_PP-OCRv3_det_infer.tar\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.00M/4.00M [00:16<00:00, 247kiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download https://paddleocr.bj.bcebos.com/PP-OCRv3/english/en_PP-OCRv3_rec_infer.tar to /root/.paddleocr/whl/rec/en/en_PP-OCRv3_rec_infer/en_PP-OCRv3_rec_infer.tar\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.96M/9.96M [00:20<00:00, 493kiB/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_infer.tar to /root/.paddleocr/whl/cls/ch_ppocr_mobile_v2.0_cls_infer/ch_ppocr_mobile_v2.0_cls_infer.tar\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2.19M/2.19M [00:15<00:00, 140kiB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023/02/11 12:00:09] ppocr DEBUG: Namespace(alpha=1.0, benchmark=False, beta=1.0, cls_batch_num=6, cls_image_shape='3, 48, 192', cls_model_dir='/root/.paddleocr/whl/cls/ch_ppocr_mobile_v2.0_cls_infer', cls_thresh=0.9, cpu_threads=10, crop_res_save_dir='./output', det=True, det_algorithm='DB', det_box_type='quad', det_db_box_thresh=0.6, det_db_score_mode='fast', det_db_thresh=0.3, det_db_unclip_ratio=1.5, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_east_score_thresh=0.8, det_limit_side_len=960, det_limit_type='max', det_model_dir='/root/.paddleocr/whl/det/en/en_PP-OCRv3_det_infer', det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, det_pse_thresh=0, det_sast_nms_thresh=0.2, det_sast_score_thresh=0.5, draw_img_save_dir='./inference_results', drop_score=0.5, e2e_algorithm='PGNet', e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_limit_side_len=768, e2e_limit_type='max', e2e_model_dir=None, e2e_pgnet_mode='fast', e2e_pgnet_score_thresh=0.5, e2e_pgnet_valid_set='totaltext', enable_mkldnn=False, fourier_degree=5, gpu_mem=500, help='==SUPPRESS==', image_dir=None, image_orientation=False, ir_optim=True, kie_algorithm='LayoutXLM', label_list=['0', '180'], lang='en', layout=True, layout_dict_path=None, layout_model_dir=None, layout_nms_threshold=0.5, layout_score_threshold=0.5, max_batch_size=10, max_text_length=25, merge_no_span_structure=True, min_subgraph_size=15, mode='structure', ocr=True, ocr_order_method=None, ocr_version='PP-OCRv3', output='./output', page_num=0, precision='fp32', process_id=0, re_model_dir=None, rec=True, rec_algorithm='SVTR_LCNet', rec_batch_num=6, rec_char_dict_path='/usr/local/lib/python3.8/dist-packages/paddleocr/ppocr/utils/en_dict.txt', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_model_dir='/root/.paddleocr/whl/rec/en/en_PP-OCRv3_rec_infer', recovery=False, save_crop_res=False, save_log_path='./log_output/', scales=[8, 16, 32], ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ser_model_dir=None, show_log=True, sr_batch_num=1, sr_image_shape='3, 32, 128', sr_model_dir=None, structure_version='PP-StructureV2', table=True, table_algorithm='TableAttn', table_char_dict_path=None, table_max_len=488, table_model_dir=None, total_process_num=1, type='ocr', use_angle_cls=False, use_dilation=False, use_gpu=False, use_mp=False, use_npu=False, use_onnx=False, use_pdf2docx_api=False, use_pdserving=False, use_space_char=True, use_tensorrt=False, use_visual_backbone=True, use_xpu=False, vis_font_path='./doc/fonts/simfang.ttf', warmup=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "ocr = PaddleOCR(use_angle_cls=False, lang='en', ocr_version = 'PP-OCRv3')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aXTtADs9XMg"
      },
      "outputs": [],
      "source": [
        "main_folder = \"/content/drive/Shareddrives/Netra_monitor_images_deployment_group/submissions/team_61 (1)\"\n",
        "os.chdir(main_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sy-nFk20m_D-"
      },
      "outputs": [],
      "source": [
        "segmentation_model = keras.models.load_model(main_folder + \"/segmentation_new_on_grayscale.h5\")\n",
        "\n",
        "classification_model = keras.models.load_model(main_folder + \"/classification_model_updated.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "WoH62KqwRk-D",
        "outputId": "091c01df-0d02-4eba-f0f7-4b12870afde6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-77191d29ee74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0myolo_monitor_main_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain_folder\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/full_yolo_folder/yolo_monitor_detection/yolov7\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myolo_monitor_main_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mattempt_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLoadStreams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLoadImages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/Shareddrives/Netra_monitor_images_deployment_group/submissions/team_61 (1)/full_yolo_folder/yolo_monitor_detection/yolov7'"
          ]
        }
      ],
      "source": [
        "yolo_monitor_main_folder = main_folder + \"/full_yolo_folder/yolo_monitor_detection/yolov7\"\n",
        "os.chdir(yolo_monitor_main_folder)\n",
        "print(os.getcwd())\n",
        "from models.experimental import attempt_load\n",
        "from utils.datasets import LoadStreams, LoadImages\n",
        "from utils.general import check_img_size, check_requirements, check_imshow, non_max_suppression, apply_classifier, \\\n",
        "      scale_coords, xyxy2xywh, strip_optimizer, set_logging, increment_path\n",
        "from utils.plots import plot_one_box\n",
        "from utils.torch_utils import select_device, load_classifier, time_synchronized, TracedModel\n",
        "device = select_device('')\n",
        "model_monitor = attempt_load(\"runs/train/yolov7_tiny_monitor2/weights/best.pt\", map_location = device)  # load FP32 model\n",
        "stride = int(model_monitor.stride.max())  # model stride\n",
        "imgsz = check_img_size(640, s=stride)\n",
        "old_img_w = old_img_h = imgsz\n",
        "old_img_b = 1\n",
        "os.chdir(main_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAU8iz1U72HY",
        "outputId": "70f70a5b-2a05-4c35-add2-d8a10460de98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/10PPTJ6MtxOuH89Fm9TqtBNUMmPveNEhU/inter_iit_main/full_yolo_folder/yolo_big_main_folder/yolov7\n",
            "Fusing layers... \n",
            "IDetect.fuse\n"
          ]
        }
      ],
      "source": [
        "yolo_big_main_folder = main_folder + \"/full_yolo_folder/yolo_big_main_folder/yolov7\"\n",
        "os.chdir(yolo_big_main_folder)\n",
        "print(os.getcwd())\n",
        "from models.experimental import attempt_load\n",
        "from utils.datasets import LoadStreams, LoadImages\n",
        "from utils.general import check_img_size, check_requirements, check_imshow, non_max_suppression, apply_classifier, \\\n",
        "      scale_coords, xyxy2xywh, strip_optimizer, set_logging, increment_path\n",
        "from utils.plots import plot_one_box\n",
        "from utils.torch_utils import select_device, load_classifier, time_synchronized, TracedModel\n",
        "device = select_device('')\n",
        "model_big = attempt_load(\"runs/train/yolov7_tiny_objects/weights/best.pt\", map_location = device)  # load FP32 model\n",
        "stride = int(model_big.stride.max())  # model stride\n",
        "imgsz = check_img_size(640, s=stride)\n",
        "old_img_w = old_img_h = imgsz\n",
        "old_img_b = 1\n",
        "os.chdir(main_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zLMH3UYSgXF",
        "outputId": "40a36d1f-4507-4104-b5c9-8eff6ca41787"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/10PPTJ6MtxOuH89Fm9TqtBNUMmPveNEhU/inter_iit_main/full_yolo_folder/yolo_bpl_main_folder/yolov7\n",
            "Fusing layers... \n",
            "IDetect.fuse\n"
          ]
        }
      ],
      "source": [
        "# yolo_bpl_main_folder = main_folder + \"/full_yolo_folder/yolo_bpl_main_folder/yolov7\"\n",
        "# os.chdir(yolo_bpl_main_folder)\n",
        "# print(os.getcwd())\n",
        "# from models.experimental import attempt_load\n",
        "# from utils.datasets import LoadStreams, LoadImages\n",
        "# from utils.general import check_img_size, check_requirements, check_imshow, non_max_suppression, apply_classifier, \\\n",
        "#       scale_coords, xyxy2xywh, strip_optimizer, set_logging, increment_path\n",
        "# from utils.plots import plot_one_box\n",
        "# from utils.torch_utils import select_device, load_classifier, time_synchronized, TracedModel\n",
        "# device = select_device('')\n",
        "# model_bpl = attempt_load(\"runs/train/yolov7_tiny_objects/weights/best.pt\", map_location = device)  # load FP32 model\n",
        "# stride = int(model_bpl.stride.max())  # model stride\n",
        "# imgsz = check_img_size(640, s=stride)\n",
        "# old_img_w = old_img_h = imgsz\n",
        "# old_img_b = 1\n",
        "# os.chdir(main_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYsTUb4aTZJq",
        "outputId": "ad70364e-96f7-42d7-a8d7-59e28493b1fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/10PPTJ6MtxOuH89Fm9TqtBNUMmPveNEhU/inter_iit_main/full_yolo_folder/yolo_ev10_main_folder/yolov7\n",
            "Fusing layers... \n",
            "IDetect.fuse\n"
          ]
        }
      ],
      "source": [
        "# yolo_ev10_main_folder = main_folder + \"/full_yolo_folder/yolo_ev10_main_folder/yolov7\"\n",
        "# os.chdir(yolo_ev10_main_folder)\n",
        "# print(os.getcwd())\n",
        "# from models.experimental import attempt_load\n",
        "# from utils.datasets import LoadStreams, LoadImages\n",
        "# from utils.general import check_img_size, check_requirements, check_imshow, non_max_suppression, apply_classifier, \\\n",
        "#       scale_coords, xyxy2xywh, strip_optimizer, set_logging, increment_path\n",
        "# from utils.plots import plot_one_box\n",
        "# from utils.torch_utils import select_device, load_classifier, time_synchronized, TracedModel\n",
        "# device = select_device('')\n",
        "# model_ev10 = attempt_load(\"runs/train/yolov7_tiny_objects/weights/best.pt\", map_location = device)  # load FP32 model\n",
        "# stride = int(model_ev10.stride.max())  # model stride\n",
        "# imgsz = check_img_size(640, s=stride)\n",
        "# old_img_w = old_img_h = imgsz\n",
        "# old_img_b = 1\n",
        "# os.chdir(main_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJjZkOmQTmtl",
        "outputId": "c9d1e904-21b1-4327-95df-cca9421cc069"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/10PPTJ6MtxOuH89Fm9TqtBNUMmPveNEhU/inter_iit_main/full_yolo_folder/yolo_ev100_main_folder/yolov7\n",
            "Fusing layers... \n",
            "IDetect.fuse\n"
          ]
        }
      ],
      "source": [
        "# yolo_ev100_main_folder = main_folder + \"/full_yolo_folder/yolo_ev100_main_folder/yolov7\"\n",
        "# os.chdir(yolo_ev100_main_folder)\n",
        "# print(os.getcwd())\n",
        "# from models.experimental import attempt_load\n",
        "# from utils.datasets import LoadStreams, LoadImages\n",
        "# from utils.general import check_img_size, check_requirements, check_imshow, non_max_suppression, apply_classifier, \\\n",
        "#       scale_coords, xyxy2xywh, strip_optimizer, set_logging, increment_path\n",
        "# from utils.plots import plot_one_box\n",
        "# from utils.torch_utils import select_device, load_classifier, time_synchronized, TracedModel\n",
        "# device = select_device('')\n",
        "# model_ev100 = attempt_load(\"runs/train/yolov7_tiny_objects/weights/best.pt\", map_location = device)  # load FP32 model\n",
        "# stride = int(model_ev100.stride.max())  # model stride\n",
        "# imgsz = check_img_size(640, s=stride)\n",
        "# old_img_w = old_img_h = imgsz\n",
        "# old_img_b = 1\n",
        "# os.chdir(main_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMJBImI8TnlW",
        "outputId": "ed6c9e8a-868e-4303-a9bc-f10cfba6d797"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/10PPTJ6MtxOuH89Fm9TqtBNUMmPveNEhU/inter_iit_main/full_yolo_folder/yolo_nihon_main_folder/yolov7\n",
            "Fusing layers... \n",
            "IDetect.fuse\n"
          ]
        }
      ],
      "source": [
        "# yolo_nihon_main_folder = main_folder + \"/full_yolo_folder/yolo_nihon_main_folder/yolov7\"\n",
        "# os.chdir(yolo_nihon_main_folder)\n",
        "# print(os.getcwd())\n",
        "# from models.experimental import attempt_load\n",
        "# from utils.datasets import LoadStreams, LoadImages\n",
        "# from utils.general import check_img_size, check_requirements, check_imshow, non_max_suppression, apply_classifier, \\\n",
        "#       scale_coords, xyxy2xywh, strip_optimizer, set_logging, increment_path\n",
        "# from utils.plots import plot_one_box\n",
        "# from utils.torch_utils import select_device, load_classifier, time_synchronized, TracedModel\n",
        "# device = select_device('')\n",
        "# model_nihon = attempt_load(\"runs/train/yolov7_tiny_objects/weights/best.pt\", map_location = device)  # load FP32 model\n",
        "# stride = int(model_nihon.stride.max())  # model stride\n",
        "# imgsz = check_img_size(640, s=stride)\n",
        "# old_img_w = old_img_h = imgsz\n",
        "# old_img_b = 1\n",
        "# os.chdir(main_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikJDLoKVmxCC"
      },
      "outputs": [],
      "source": [
        "# Helper Function:\n",
        "def screen_from_segmentation(image, bbox = None, segmentation_model = None):\n",
        "  def segmentation(segmentation_model, image):\n",
        "    SIZE_X = 512 #Resize images (height  = X, width = Y)\n",
        "    SIZE_Y = 512\n",
        "    image = cv2.resize(image, (SIZE_Y, SIZE_X))\n",
        "    # image = np.reshape(image, (1, 256, 256, 3))\n",
        "    image = np.reshape(image, (1, 512, 512, 1))\n",
        "    pred_mask = segmentation_model.predict(image)\n",
        "    pred_mask = np.reshape(pred_mask, (512, 512, 1))\n",
        "    pred_mask = cv2.resize(pred_mask, dsize=(1280, 720), interpolation=cv2.INTER_CUBIC)\n",
        "    # pred_mask = np.reshape(pred_mask, (720, 1280, 1))\n",
        "    # cv2.imwrite('/content/drive/MyDrive/InterIIT-PS2-MID-PREP/segmented_images/image'+\"bad1\" + '.jpg', pred_mask)\n",
        "    return pred_mask\n",
        "\n",
        "  gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "  pred_mask = segmentation(segmentation_model, gray)\n",
        "\n",
        "\n",
        "  # Functions to WARP Images from detected masks\n",
        "  # function to rearrange the detected corners since they are not in any conventional order\n",
        "  def order_points(pts):\n",
        "      '''Rearrange coordinates to order:\n",
        "        top-left, top-right, bottom-right, bottom-left'''\n",
        "      rect = np.zeros((4, 2), dtype='float32')\n",
        "      pts = np.array(pts)\n",
        "      s = pts.sum(axis=1)\n",
        "      # Top-left point will have the smallest sum.\n",
        "      rect[0] = pts[np.argmin(s)]\n",
        "      # Bottom-right point will have the largest sum.\n",
        "      rect[2] = pts[np.argmax(s)]\n",
        "\n",
        "      diff = np.diff(pts, axis=1)\n",
        "      # Top-right point will have the smallest difference.\n",
        "      rect[1] = pts[np.argmin(diff)]\n",
        "      # Bottom-left will have the largest difference.\n",
        "      rect[3] = pts[np.argmax(diff)]\n",
        "      # Return the ordered coordinates.\n",
        "      return rect.astype('int').tolist()\n",
        "\n",
        "\n",
        "  def mask_upres(mask):\n",
        "    mask = (mask*255).astype(np.uint8)\n",
        "    # Find object contours\n",
        "    cnts, hiers = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[-2:]\n",
        "    area_max = 0\n",
        "    #Find biggest contour\n",
        "    biggest_cnt = []\n",
        "    for cnt in cnts:\n",
        "        area = cv2.contourArea(cnt)\n",
        "        # print(area)\n",
        "        if (area_max < area):\n",
        "            area_max = area\n",
        "            biggest_cnt = cnt\n",
        "\n",
        "    #approximate the contour to a polygon with few vertices\n",
        "    peri = cv2.arcLength(biggest_cnt, True)\n",
        "    approx = cv2.approxPolyDP(biggest_cnt, 0.01 * peri, True)\n",
        "    # approx = cv2.approxPolyDP(biggest_cnt, 0.01 * peri, True)[:4]\n",
        "    # mask_approx = np.zeros((mask.shape[0],mask.shape[1]), dtype = np.uint8)\n",
        "    # cv2.fillPoly(mask_approx, pts =[approx], color=(255))\n",
        "    # return order_points(approx)\n",
        "    return approx\n",
        "\n",
        "  def manhattan_distance(point1, point2):\n",
        "    x1,y1 = point1\n",
        "    x2,y2 = point2\n",
        "    return np.abs(x1-x2) + np.abs(y1-y2)\n",
        "\n",
        "\n",
        "  #   return success, corners, final\n",
        "  def select_best_corners(corners, bbox):\n",
        "  # bbox will be [topleft_x, topleft_y, width, height]\n",
        "  # corners will be [[topleft],[topright],[bottomright],[bottomleft]]\n",
        "    topleft_x, topleft_y, width, height = bbox\n",
        "    yolo_corners = np.array([[topleft_x, topleft_y],[topleft_x+width,topleft_y],[topleft_x+width, topleft_y+height], [topleft_x, topleft_y+height]])\n",
        "\n",
        "    new_corners = []\n",
        "    for vertex in yolo_corners:   # yolo ke corners\n",
        "      minDist = 1e9\n",
        "      bestmatch = None\n",
        "      for corner in corners:        # mask ke corners\n",
        "        # print(\"yolo corner: \", vertex)\n",
        "        # print(\"mask corner: \", corner)\n",
        "        dist = manhattan_distance(corner, vertex)\n",
        "        # print(f\"Manhattan distance : {dist}\")\n",
        "        if dist < minDist:\n",
        "          minDist = dist\n",
        "          bestmatch = corner\n",
        "\n",
        "      new_corners.append(bestmatch)\n",
        "    return new_corners\n",
        "\n",
        "  def warp_masks(image, mask, bbox):\n",
        "    \"\"\"\n",
        "      image should be of three channels, and mask should of grayscale and values of mask should be between 0 and 1\n",
        "      returns success, corners, warped_image\n",
        "    \"\"\"\n",
        "    corners = mask_upres(mask)\n",
        "    corners = np.reshape(corners, (corners.shape[0],-1))\n",
        "\n",
        "\n",
        "    if corners.shape[0] == 4:\n",
        "      success = True\n",
        "      corners = order_points(corners)\n",
        "    elif corners.shape[0] > 4:\n",
        "      best_corners = select_best_corners(corners, bbox)\n",
        "      corners = np.array(best_corners)\n",
        "      if corners.shape[0] == 4:\n",
        "        success = True\n",
        "      else:\n",
        "        success = False\n",
        "        return success, corners, None\n",
        "      # return False, corners, None\n",
        "    elif corners.shape[0] < 4:\n",
        "      success = False\n",
        "      return success, corners, None\n",
        "\n",
        "\n",
        "    (tl, tr, br, bl) = corners\n",
        "    # Finding the maximum width.\n",
        "    widthA = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2))\n",
        "    widthB = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2))\n",
        "    maxWidth = max(int(widthA), int(widthB))\n",
        "    # Finding the maximum height.\n",
        "    heightA = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2))\n",
        "    heightB = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2))\n",
        "    maxHeight = max(int(heightA), int(heightB))\n",
        "    # Final destination co-ordinates.\n",
        "    destination_corners = [[0, 0], [maxWidth, 0], [maxWidth, maxHeight], [0, maxHeight]]\n",
        "\n",
        "\n",
        "    # Getting the homography.\n",
        "    M = cv2.getPerspectiveTransform(np.float32(corners), np.float32(destination_corners))\n",
        "    # Perspective transform using homography.\n",
        "    final = cv2.warpPerspective(image, M, (destination_corners[2][0], destination_corners[2][1]))\n",
        "\n",
        "\n",
        "    return success, corners, final\n",
        "\n",
        "  success, corners, final = warp_masks(image, pred_mask, bbox)\n",
        "  return success, corners, final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wN0XHiNZpdDm"
      },
      "outputs": [],
      "source": [
        "def identify_screen_type(screen,classification_model = classification_model, show_image = False):\n",
        "  screen = cv2.resize(screen, dsize = (512, 512))\n",
        "  if show_image:\n",
        "    plt.imshow(screen)\n",
        "  screen = np.reshape(screen, (1,512,512,3))\n",
        "  prediction = np.argmax(classification_model.predict(screen))\n",
        "  return prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFRPPBnXkrd8"
      },
      "outputs": [],
      "source": [
        "def create_graph_from_image(img):\n",
        "\n",
        "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    ret, im = cv2.threshold(img_gray, 120, 255, cv2.THRESH_BINARY)\n",
        "    contours, hierarchy  = cv2.findContours(im, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    largest_contour = max(contours, key=cv2.contourArea)\n",
        "    data_points = np.squeeze(largest_contour)\n",
        "    greatest = max(data_points[:,1])\n",
        "\n",
        "    df = pd.DataFrame(data_points, columns = ['X','Y'])\n",
        "    data_points = df.groupby('X').mean()\n",
        "    data_points = data_points.reset_index()\n",
        "    data_points = data_points.values\n",
        "\n",
        "    # print(data_points)\n",
        "\n",
        "    return data_points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TAcw7vMCcWFa"
      },
      "outputs": [],
      "source": [
        "#input is numpy image (720,1280,3) and 2d array of all bboxes\n",
        "def detect_ocr(img, all_bbox):\n",
        "  map_arr = ['HR', 'HR_W', 'BP', 'SPO2', 'RR']\n",
        "  img_output = {}\n",
        "  hr_w_data_pts = []\n",
        "  for bbox in all_bbox:\n",
        "    og_img = img\n",
        "    label = map_arr[bbox[-1]]\n",
        "    x_top = int(bbox[0] - bbox[2]/2)\n",
        "    y_top = int(bbox[1] - bbox[3]/2)\n",
        "    width = int(bbox[2])\n",
        "    height = int(bbox[3])\n",
        "\n",
        "\n",
        "    # print(y_top, y_top + height, x_top, x_top+width, og_img.shape)\n",
        "\n",
        "    img_to_ocr = og_img[y_top:y_top+height,x_top: x_top+width, :]\n",
        "\n",
        "    # print(y_top, y_top + height, x_top, x_top+width, img_to_ocr.shape)\n",
        "\n",
        "    #print(label, \"working\")\n",
        "    if label == \"HR_W\":\n",
        "      # call graph code\n",
        "      hr_w_data_pts = create_graph_from_image(img_to_ocr)\n",
        "\n",
        "    else:\n",
        "\n",
        "      result = ocr.ocr(img_to_ocr, cls = False)\n",
        "\n",
        "      # resul = result[0]\n",
        "      # image = img\n",
        "      # boxes = [line[0] for line in resul]\n",
        "      # txts = [line[1][0] for line in resul]\n",
        "      # scores = [line[1][1] for line in resul]\n",
        "      # im_show = draw_ocr(image, boxes, txts, scores, font_path = '/content/drive/MyDrive/inter_iit_main_folder/font_fille/ShortBaby-Mg2w.ttf')\n",
        "      # final = cv2.resize(im_show, [3000,1000])\n",
        "      # print(label)\n",
        "      # cv2_imshow(final)\n",
        "\n",
        "\n",
        "      area = 0\n",
        "      for r in result:\n",
        "        for line in r:\n",
        "          bbox  = line[0]\n",
        "          width = bbox[1][0] - bbox[0][0]\n",
        "          height = bbox[3][1] - bbox[0][1]\n",
        "          rec_area = width*height\n",
        "          if(rec_area > area):\n",
        "            area = rec_area\n",
        "            prediction = line[1][0]\n",
        "          #preprocessing\n",
        "          img_output[label] = prediction\n",
        "\n",
        "          if label == \"BP\":\n",
        "            if '(' in line[1][0]:\n",
        "              MAP = line[1][0].split('(')[1]\n",
        "              MAP = MAP.split(')')[0]\n",
        "\n",
        "              img_output[\"MAP\"] = MAP\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      # bbox_area = width * height\n",
        "      # area = 0\n",
        "      # for r in result:\n",
        "      #   for line in r:\n",
        "      #     bbox  = line[0]\n",
        "      #     width = bbox[1][0] - bbox[0][0]\n",
        "      #     height = bbox[3][1] - bbox[0][1]\n",
        "      #     rec_area = width*height\n",
        "      #     if(rec_area > area):\n",
        "      #       area = rec_area\n",
        "      #       prediction = line[1][0]\n",
        "      #     #preprocessing\n",
        "      #     if True:\n",
        "      #       if(area> 0.15*bbox_area):\n",
        "      #         if(prediction.isnumeric()):\n",
        "      #           img_output[label] = prediction\n",
        "      #         else:\n",
        "      #           for i in range(len(prediction) - 3):\n",
        "      #             if(prediction[i:i+3].isnumeric()):\n",
        "      #               img_output[label] = prediction[i:i+3]\n",
        "\n",
        "      #           for i in range(len(prediction) - 2):\n",
        "      #             if(prediction[i:i+2].isnumeric()):\n",
        "      #               img_output[label] = prediction[i:i+2]\n",
        "\n",
        "  return hr_w_data_pts, img_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-X90pbnKQ-VA"
      },
      "outputs": [],
      "source": [
        "#img input size(720,1280,3)\n",
        "def predict(img_path, model):\n",
        "    dataset = LoadImages(img_path, img_size=imgsz, stride=stride)\n",
        "    for path, i, im0s, vid_cap in dataset:\n",
        "      img = torch.from_numpy(i).to(device)\n",
        "      img = img.float()\n",
        "      img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
        "      if img.ndimension() == 3:\n",
        "        img = img.unsqueeze(0)\n",
        "      # print(img.shape)\n",
        "      with torch.no_grad():   # Calculating gradients would cause a GPU memory leak\n",
        "        pred = model(img)[0]\n",
        "      pred = non_max_suppression(pred)\n",
        "\n",
        "      all_box = []\n",
        "\n",
        "      for i, det in enumerate(pred):\n",
        "        p, s, im0, frame = path, '', im0s, getattr(dataset, 'frame', 0)\n",
        "        gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]\n",
        "\n",
        "        if len(det):\n",
        "          det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n",
        "          # print(det)\n",
        "          for c in det[:, -1].unique():\n",
        "            n = (det[:, -1] == c).sum()  # detections per class\n",
        "            # print(n)\n",
        "            #s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n",
        "          for *xyxy, conf, cls in reversed(det):\n",
        "            # print(xyxy)\n",
        "            # print(cls)\n",
        "            xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()\n",
        "            # print(\"xywh =\",xywhh)\n",
        "            xywh[0] = xywh[0] * 1280\n",
        "            xywh[2] = xywh[2] * 1280\n",
        "            xywh[1] = xywh[1] * 720\n",
        "            xywh[3] = xywh[3] * 720\n",
        "            xywh.append(int(cls.cpu().numpy()))\n",
        "            all_box.append(xywh)\n",
        "    return all_box"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mxqIfowOwWu"
      },
      "outputs": [],
      "source": [
        "def inference(image_path:str)-> dict:\n",
        "\n",
        "  main_folder = os.getcwd()\n",
        "\n",
        "  img_name = image_path.split('/')[-1]\n",
        "\n",
        "  main_yolo_folder_path = main_folder + '/' + \"full_yolo_folder\"\n",
        "\n",
        "  main_output_folder = main_folder + '/' + 'outputs'\n",
        "\n",
        "  if not os.path.exists(main_output_folder):\n",
        "    os.mkdir(main_output_folder)\n",
        "\n",
        "  img_output_folder = main_output_folder + '/' + img_name[:-5]\n",
        "\n",
        "  if not os.path.exists(img_output_folder):\n",
        "    os.mkdir(img_output_folder)\n",
        "\n",
        "  main_yolo_outputs_folder = img_output_folder + '/' + \"yolo_mon_outputs\"\n",
        "\n",
        "  if not os.path.exists(main_yolo_outputs_folder):\n",
        "    os.mkdir(main_yolo_outputs_folder)\n",
        "\n",
        "  main_seg_outputs_folder = img_output_folder + '/' + \"seg_mon_outputs\"\n",
        "\n",
        "  if not os.path.exists(main_seg_outputs_folder):\n",
        "    os.mkdir(main_seg_outputs_folder)\n",
        "\n",
        "  main_obj_detect_folder = img_output_folder + '/' + \"obj_dec_outputs\"\n",
        "\n",
        "  if not os.path.exists(main_obj_detect_folder):\n",
        "    os.mkdir(main_obj_detect_folder)\n",
        "\n",
        "  mon_bbox_out = predict(image_path, model_monitor)\n",
        "  if len(mon_bbox_out) == 0:\n",
        "    print(\"Monitor not detected\")\n",
        "    return {}\n",
        "\n",
        "  bbox_coords = mon_bbox_out[0]\n",
        "  x_center = bbox_coords[0]\n",
        "  y_center = bbox_coords[1]\n",
        "  width = bbox_coords[2]\n",
        "  height = bbox_coords[3]\n",
        "  classi_list = [int(int(x_center) - int(width)/2), int(int(y_center) - int(height)/2),\n",
        "                 int(width), int(height)]\n",
        "\n",
        "  og_img = cv2.imread(main_folder + '/' + image_path)\n",
        "  mon_bbox_img = og_img[classi_list[1]: classi_list[1] + classi_list[3],\n",
        "                        classi_list[0]: classi_list[0] + classi_list[2], :]\n",
        "\n",
        "  mon_bbox_img = cv2.resize(mon_bbox_img, (1280, 720))\n",
        "  cv2.imwrite(main_yolo_outputs_folder + '/' + 'yolo_mon_' + img_name, mon_bbox_img)\n",
        "\n",
        "  success , corners, warp = screen_from_segmentation(og_img, classi_list, segmentation_model)\n",
        "  warp = cv2.resize(warp, (1280, 720))\n",
        "  cv2.imwrite(main_seg_outputs_folder + '/' + 'seg_mon_' + img_name, warp)\n",
        "\n",
        "  map_arr = ['HR', 'HR_W', 'BP', 'SPO2', 'RR']\n",
        "\n",
        "  og_img = mon_bbox_img\n",
        "\n",
        "  if success:\n",
        "    mon_class = identify_screen_type(warp, classification_model, show_image = False)\n",
        "    og_img = warp\n",
        "    yolo_pred_2 = predict(main_seg_outputs_folder + '/' + 'seg_mon_' + img_name, model_big)\n",
        "    hr_w_data_pts, result = detect_ocr(warp, yolo_pred_2)\n",
        "    if result == {}:\n",
        "      mon_class = identify_screen_type(mon_bbox_img, classification_model, show_image = False)\n",
        "      og_img = mon_bbox_img\n",
        "      yolo_pred_2 = predict(main_yolo_outputs_folder + '/' + 'yolo_mon_' + img_name, model_big)\n",
        "      hr_w_data_pts, result = detect_ocr(mon_bbox_img, yolo_pred_2)\n",
        "  else:\n",
        "    mon_class = identify_screen_type(mon_bbox_img, classification_model, show_image = False)\n",
        "    og_img = mon_bbox_img\n",
        "    yolo_pred_2 = predict(main_yolo_outputs_folder + '/' + 'yolo_mon_' + img_name, model_big)\n",
        "    hr_w_data_pts, result = detect_ocr(mon_bbox_img, yolo_pred_2)\n",
        "\n",
        "  for i in yolo_pred_2:\n",
        "    label = map_arr[int(i[-1])]\n",
        "    x_top = int(i[0] - i[2]/2)\n",
        "    y_top = int(i[1] - i[3]/2)\n",
        "    width = int(i[2])\n",
        "    height = int(i[3])\n",
        "    bbox_img = og_img[y_top:y_top+height,x_top: x_top+width, :]\n",
        "    cv2.imwrite(main_obj_detect_folder + '/' + label + '_' + img_name, bbox_img)\n",
        "    if label == 'HR_W':\n",
        "      if hr_w_data_pts != []:\n",
        "        plt.figure(figsize=(20,10))\n",
        "        plt.plot(hr_w_data_pts[:, 0], abs(max(hr_w_data_pts[:,1])-hr_w_data_pts[:, 1]), label = \"Heart Rate\")\n",
        "        plt.grid()\n",
        "        plt.xlabel(\"X\")\n",
        "        plt.ylabel(\"Y\")\n",
        "        plt.title(\"ECG Graph\")\n",
        "        plt.legend()\n",
        "        plt.savefig(main_obj_detect_folder + '/' + label + '_' + img_name[:-5] + '.png')\n",
        "\n",
        "  if 'BP' in result.keys():\n",
        "    if '/' in result['BP']:\n",
        "      sbp = result['BP'].split('/')[0]\n",
        "      dbp = result['BP'].split('/')[1]\n",
        "\n",
        "      if (len(sbp) != 0) :\n",
        "        result['SBP'] = sbp\n",
        "      if(len(dbp)!= 0):\n",
        "        result['DBP'] = dbp\n",
        "      try:\n",
        "        False\n",
        "        # result['MAP'] = str(int((int(sbp) + int(dbp))/2))\n",
        "      except:\n",
        "        if ' ' in dbp:\n",
        "          result['DBP'] = dbp.split(' ')[0]\n",
        "          # result['MAP'] = dbp.split(' ')[1]\n",
        "\n",
        "      del result['BP']\n",
        "    else:\n",
        "      sbp = result['BP']\n",
        "      if(len(sbp) > 3 and sbp.isnumeric()):\n",
        "        l = int(len(sbp)/2)\n",
        "        dbp = sbp[-l:]\n",
        "        result['DBP'] = dbp\n",
        "        sbp = sbp[:-l]\n",
        "      result['SBP'] = sbp\n",
        "      del result['BP']\n",
        "\n",
        "  os.chdir(main_folder)\n",
        "\n",
        "  return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69t6eihgB-mw"
      },
      "source": [
        "#### Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-B3qjbmZyRpX"
      },
      "outputs": [],
      "source": [
        "test_folder = \"inputs\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CVb0ELYyaIp"
      },
      "outputs": [],
      "source": [
        "for i in os.listdir(test_folder):\n",
        "  print('--------------------------------------------------------------------------------------------------')\n",
        "  print(\"IMG PATH :\",test_folder+'/'+i)\n",
        "  img = cv2.imread(test_folder+'/'+i)\n",
        "  res = inference(test_folder+'/'+i)\n",
        "  cv2_imshow(img)\n",
        "  print(res)\n",
        "  if os.path.exists(main_folder + f'/outputs/{i[:-5]}/obj_dec_outputs/HR_W_{i[:-5]}.png'):\n",
        "    print(\"Path exists\", main_folder + f'/outputs/{i[:-5]}/obj_dec_outputs/HR_W_{i[:-5]}.png')\n",
        "    img_2 = cv2.imread(main_folder + f'/outputs/{i[:-5]}/obj_dec_outputs/HR_W_{i[:-5]}.png')\n",
        "    cv2_imshow(img_2)\n",
        "  print('--------------------------------------------------------------------------------------------------')\n",
        "  print()\n",
        "  print()\n",
        "  print()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UKf502dtQr8C"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}